import os, csv, time, datetime, pprint, sys, urllib.request, urllib.error
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
 
#sys.path.append('\study')
#def method(val):
#    print(val)

class UserSide:
    def __init__(self, search_url, select_area,  csv_name):
        self.url = search_url
        self.area = select_area
        self.csv = csv_name
    
    def base_url(self): # 検索url
        return self.url
    
    def search_area(self): # 検索地域
        return self.area
    
    def search_keyword(self): # 検索キーワード
        return self.key
    
    def output_csv(self): # 選択する地域のhtmlからidを選択
        return self.csv
    
class BrowserSide:
    def __init__(self, area_input, area_class, search_btn_id, nextpage_btn_id):
        self.area_input = area_input
        self.area_class = area_class
        self.search_btn_id = search_btn_id
        self.nextpage_btn_id = nextpage_btn_id
    
    def area_input_select(self): # htmlから選択した地域のidを選択
        return self.area_input
    
    def key_input_select(self): # htmlから選択した地域のidを選択
        return self.key_input
    
    def area_class_select(self): # htmlから選択した地域のidを選択
        return self.area_class
    
    def key_id_select(self): # htmlから選択したキーワードのidを選択
        return self.keyword_id
    
    def search_id_select(self): # htmlから検索ボタンのidを選択
        return self.search_btn_id
    
    def nextpage_id_select(self): # htmlから次のページボタンのidを選択
        return self.nextpage_btn_id

def bs_search():
    page = 1
    while True:
        if len(driver.find_elements_by_class_name(tabe_html.nextpage_id_select())) > 0:
            print("##############################page: {} ##############################".format(page))
            driver.implicitly_wait(10)
            page_source = driver.page_source
            bs = BeautifulSoup(page_source, 'html.parser')

            shop_id = []
            shop_name = []
            area_genre = []
            shop_url = [] 
            shop_rate = [] 
            shop_review_cnt = []
            data_list = [
                shop_id , shop_name, area_genre, shop_url, shop_rate, shop_review_cnt
            ]
            osaki_shop_list = bs.find_all('li', class_='js-rst-cassette-wrap')
            for lists in osaki_shop_list:
                data_id = lists.get('data-rst-id')
                shop_id.append(data_id)

                shop_names = lists.find('a', class_='cpy-rst-name')
                shop_name.append(shop_names.text)

                area_genres = lists.find('span', class_='list-rst__area-genre')
                area_genre.append(area_genres.text)

                shop_urls = lists.find('a', class_='list-rst__rst-name-target')
                href = shop_urls.get('href')
                shop_url.append(href)

                shop_rates = lists.find('span', class_='list-rst__rating-val')
                if shop_rates is None:
                    shop_rate.append('-')
                else:
                    shop_rate.append(shop_rates.text)

                review_count = lists.find('a', class_='list-rst__rvw-count-target')
                if review_count is None:
                    shop_review_cnt.append('-')
                else:
                    shop_review_cnt.append(review_count.text)            

            tabelog_done = pd.DataFrame(data_list)
            #df = df.append(tabelog_done, ignore_index=True)

            print(tabelog_done)

            nextpage = driver.find_element_by_class_name(tabe_html.nextpage_id_select()).get_attribute("href")
            driver.get(nextpage)
            page += 1
            driver.implicitly_wait(10)
            time.sleep(5)
        else:
            print('##################################################Last page##################################################')
            page_source = driver.page_source
            bs = BeautifulSoup(page_source, 'html.parser')

            shop_id = []
            shop_name = []
            area_genre = [] 
            shop_url = [] 
            shop_rate = [] 
            shop_review_cnt = []
            data_list = [
                shop_id , shop_name, area_genre, shop_url, shop_rate, shop_review_cnt
            ]
            osaki_shop_list = bs.find_all('li', class_='js-rst-cassette-wrap')
            for lists in osaki_shop_list:
                data_id = lists.get('data-rst-id')
                shop_id.append(data_id)

                shop_names = lists.find('a', class_='cpy-rst-name')
                shop_name.append(shop_names.text)

                area_genres = lists.find('span', class_='list-rst__area-genre')
                area_genre.append(area_genres.text)

                shop_urls = lists.find('a', class_='list-rst__rst-name-target')
                href = shop_urls.get('href')
                shop_url.append(href)

                shop_rates = lists.find('span', class_='list-rst__rating-val')
                if shop_rates is None:
                    shop_rate.append('-')
                else:
                    shop_rate.append(shop_rates.text)

                review_count = lists.find('a', class_='list-rst__rvw-count-target')
                if review_count is None:
                    shop_review_cnt.append('-')
                else:
                    shop_review_cnt.append(review_count.text)

            tabelog_done = pd.DataFrame(data_list)
            #df = df.append(tabelog_done, ignore_index=True)

            print(tabelog_done)
            break
            
        data_list_n = np.array(tabelog_done)        
        writer.writerows(data_list_n.T)
            
    data_list_n = np.array(tabelog_done)        
    writer.writerows(data_list_n.T)


#area list 山手線
osaki = {'ファイル名':'osaki', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[4]', '検索':'大崎'}
gotanda = {'ファイル名':'gotanda', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'五反田'}
meguro = {'ファイル名':'meguro', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'目黒'}
ebisu = {'ファイル名':'ebisu', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'恵比寿'}
shibuya = {'ファイル名':'shibuya', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'渋谷'}
harajuku = {'ファイル名':'harajuku', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'原宿'}
yoyogi = {'ファイル名':'yoyogi', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'代々木'}
shinjuku = {'ファイル名':'shinjuku', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'新宿'}
shinookubo = {'ファイル名':'shinookubo', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'新大久保'}
takadanobaba = {'ファイル名':'takadanobaba', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'高田馬場'}
mejiro = {'ファイル名':'mejiro', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'目白'}
ikebukuro = {'ファイル名':'ikebukuro', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'池袋'}
ootsuka = {'ファイル名':'ootsuka', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'大塚'}
sugamo = {'ファイル名':'sugamo', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'巣鴨'}
komagome = {'ファイル名':'komagome', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'駒込'}
nishinippori = {'ファイル名':'nishinippori', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'西日暮里'}
nippori = {'ファイル名':'nippori', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'日暮里'}
uguisudani = {'ファイル名':'uguisudani', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'鶯谷'}
ueno = {'ファイル名':'ueno', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'上野'}
okachimachi = {'ファイル名':'okachimachi', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'御徒町'}
akihabara = {'ファイル名':'akihabara', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'秋葉原'}
kanda = {'ファイル名':'kanda', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'神田'}
tokyo = {'ファイル名':'tokyo', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[2]', '検索':'東京'}
yurakucho = {'ファイル名':'yurakucho', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'有楽町'}
shinbashi = {'ファイル名':'shinbashi', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'新橋'}
hamamatsucho = {'ファイル名':'hamamatsucho', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'浜松町'}
tamachi = {'ファイル名':'tamachi', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'田町'}
shinagawa = {'ファイル名':'shinagawa', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'品川'}
musasisakai = {'ファイル名':'musasisakai', 'list-id選択':'//*[@id="rstsearch_form"]/div/div[5]/div/ul/li[1]', '検索':'武蔵境'}

    
# ブラウザを立ち上がらないように設定
browser_path = "C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"
chromedriver_path = r'C:\Users\study\Desktop\chromedriver_win32\chromedriver.exe' 
options = Options()
options.binary_location = browser_path # chromeのパスを指定
#options.add_argument('--headless') # ヘッドレスモードを有効にする（コメントアウトするとブラウザ表示）
driver = webdriver.Chrome(chrome_options = options, executable_path=chromedriver_path) # webdriverのパス指定

# サーバーに負担をかけないように待機 implicitly_wait(秒)
driver.implicitly_wait(5)

# UserSide("食べログURL", "検索するエリア", "検索するキーワード", "csvのファイル名(エリアを英語で)") 
# 編集するところはエリアとキーワード、csvファイル名
tabe_input = UserSide("https://tabelog.com/", gotanda['検索'], gotanda['ファイル名'])

# 編集するところは "ui-id-6", "ui-id-14"
# エリア、キーワードを入力するとリストが表示され、そのリストから検索したいエリア（キーワード）を選択する
# エリアのリストは"ui-id-3"が一番上にくる
# キーワードのリストは"ui-id-13"が一番上にくる
tabe_html = BrowserSide("sa", gotanda['list-id選択'], "js-global-search-btn", "c-pagination__arrow--next")

#本日の日付
now = datetime.datetime.now()
#ファイルの存在をチェック
#ファイルがない場合(False)スクレイピングしてファイル作成&保存
if(os.path.isfile(r'C:\Users\study\python_study\tabelog_' + tabe_input.output_csv() + '_{0:%Y%m%d}.csv'.format(now)) == False):
    
    # ブラウザでページを開く
    driver.get(tabe_input.base_url())

    
    #############################地域#############################
    # 検索地域入力欄に指定したキーワード入力
    elem = driver.find_element_by_id(tabe_html.area_input_select())
    elem.clear()
    elem.send_keys(tabe_input.search_area())

    # ドロップダウンリストから選んで選択
    elem = driver.find_element_by_xpath(tabe_html.area_class_select())
    elem.click()
    ##############################################################

    ########################キーワード#############################
    # 検索地域入力欄に指定したキーワード入力
    #elem = driver.find_element_by_id(tabe_html.key_input_select())
    #elem.clear()
    #elem.send_keys(tabe_input.search_keyword())

    # ドロップダウンリストから選んで選択
    #elem = driver.find_element_by_id(tabe_html.key_id_select())
    #elem.click()
    ##############################################################

    # 検索ボタンをクリック
    elem = driver.find_element_by_id(tabe_html.search_id_select())
    elem.click()

    driver.implicitly_wait(10)


    # スクレイピング
    with open(r'C:\Users\study\python_study\tabelog_' + tabe_input.output_csv() + '_{0:%Y%m%d}.csv'.format(now), 'w', encoding='utf_8_sig') as f:
        header =['id', '店舗名', 'エリア/ジャンル', '店舗URL', '評価点', '口コミ数']
        writer = csv.writer(f, lineterminator='\n')
        writer.writerow(header)

        bs_search()
#ファイルがある場合(True)、プリントする
else:
    print('ファイルが存在します')
    pass
time.sleep(5)

print("DONE")
